{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U peft\n!pip install -U bitsandbytes\n!pip install -U accelerate\n!pip install -U \"transformers>=4.42.3\"","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:05:05.179195Z","iopub.execute_input":"2024-11-08T00:05:05.179551Z","iopub.status.idle":"2024-11-08T00:06:11.225767Z","shell.execute_reply.started":"2024-11-08T00:05:05.179516Z","shell.execute_reply":"2024-11-08T00:06:11.224756Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-1.1.1\nRequirement already satisfied: transformers>=4.42.3 in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers>=4.42.3\n  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.42.3) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (2024.8.30)\nDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.46.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport copy\nimport torch\nimport numpy as np\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom sklearn.metrics import (\n    log_loss, \n    accuracy_score\n)\nfrom transformers import (\n    BitsAndBytesConfig,\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    Gemma2Config,\n    PreTrainedTokenizerBase, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import (\n    LoraConfig, \n    get_peft_model, \n    prepare_model_for_kbit_training, \n    TaskType\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-08T00:06:11.227843Z","iopub.execute_input":"2024-11-08T00:06:11.228218Z","iopub.status.idle":"2024-11-08T00:06:32.461350Z","shell.execute_reply.started":"2024-11-08T00:06:11.228176Z","shell.execute_reply":"2024-11-08T00:06:32.460536Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"#### <font color='red'> dataclass </font>","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    output_dir: str = \"output\"\n    model_ckpt: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n    max_length: int = 1024\n    n_splits: int = 5\n    fold_idx: int = 0\n    optim_type: str = \"adamw_8bit\"\n    per_device_train_batch_size: int = 2\n    gradient_accumulation_steps: int = 2  # global batch size is 8 \n    per_device_eval_batch_size: int = 8\n    n_epochs: int = 1\n    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n    lr: float = 2e-4\n    warmup_steps: int = 20\n    lora_r: int = 16\n    lora_alpha: float = lora_r * 2\n    lora_dropout: float = 0.05\n    lora_bias: str = \"none\"\n    \nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:06:32.462464Z","iopub.execute_input":"2024-11-08T00:06:32.463115Z","iopub.status.idle":"2024-11-08T00:06:32.470913Z","shell.execute_reply.started":"2024-11-08T00:06:32.463078Z","shell.execute_reply":"2024-11-08T00:06:32.469860Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"config.lora_dropout","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:06:32.473369Z","iopub.execute_input":"2024-11-08T00:06:32.473811Z","iopub.status.idle":"2024-11-08T00:06:32.488530Z","shell.execute_reply.started":"2024-11-08T00:06:32.473764Z","shell.execute_reply":"2024-11-08T00:06:32.487701Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0.05"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"output\",\n    overwrite_output_dir=True,\n    report_to=\"none\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"steps\",\n    save_steps=200,\n    optim=config.optim_type,\n    fp16=True,\n    learning_rate=config.lr,\n    warmup_steps=config.warmup_steps,\n)\nlora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:06:32.489712Z","iopub.execute_input":"2024-11-08T00:06:32.490000Z","iopub.status.idle":"2024-11-08T00:06:32.608409Z","shell.execute_reply.started":"2024-11-08T00:06:32.489968Z","shell.execute_reply":"2024-11-08T00:06:32.607609Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.model_ckpt)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:06:32.609453Z","iopub.execute_input":"2024-11-08T00:06:32.609767Z","iopub.status.idle":"2024-11-08T00:06:35.352979Z","shell.execute_reply.started":"2024-11-08T00:06:32.609733Z","shell.execute_reply":"2024-11-08T00:06:35.352133Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe70e92775e74ebda2a1dc6be2dc773a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f1a292ac07490b933c008273ccba28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a31ab7383e4365898114954b5ce908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791f399d7e204c06af43cf8bd4a7463f"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model = Gemma2ForSequenceClassification.from_pretrained(\n    config.model_ckpt,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:06:35.354098Z","iopub.execute_input":"2024-11-08T00:06:35.354391Z","iopub.status.idle":"2024-11-08T00:09:06.769808Z","shell.execute_reply.started":"2024-11-08T00:06:35.354352Z","shell.execute_reply":"2024-11-08T00:09:06.769002Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3616b69914e8422ba52cb82377fcbc8d"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f059af37334f7e86bf024b0cbca2a9"}},"metadata":{}},{"name":"stderr","text":"Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-9b-it-bnb-4bit and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:09:30.748844Z","iopub.execute_input":"2024-11-08T00:09:30.749757Z","iopub.status.idle":"2024-11-08T00:09:30.762221Z","shell.execute_reply.started":"2024-11-08T00:09:30.749711Z","shell.execute_reply":"2024-11-08T00:09:30.761236Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 7,891,456 || all params: 9,249,608,192 || trainable%: 0.0853\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"ds = Dataset.from_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\nds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:09:33.737494Z","iopub.execute_input":"2024-11-08T00:09:33.738275Z","iopub.status.idle":"2024-11-08T00:09:37.517213Z","shell.execute_reply.started":"2024-11-08T00:09:33.738235Z","shell.execute_reply":"2024-11-08T00:09:37.516323Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0744b0a57b96409cbe291551b88ab824"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:09:40.990939Z","iopub.execute_input":"2024-11-08T00:09:40.991360Z","iopub.status.idle":"2024-11-08T00:09:40.997437Z","shell.execute_reply.started":"2024-11-08T00:09:40.991319Z","shell.execute_reply":"2024-11-08T00:09:40.996598Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n    num_rows: 100\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def process_text(text: str) -> str:\n    return \" \".join(eval(text, {\"null\": \"\"}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:10:05.737796Z","iopub.execute_input":"2024-11-08T00:10:05.738663Z","iopub.status.idle":"2024-11-08T00:10:05.743079Z","shell.execute_reply.started":"2024-11-08T00:10:05.738621Z","shell.execute_reply":"2024-11-08T00:10:05.742147Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"text = ds['prompt'][0]\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:10:49.176309Z","iopub.execute_input":"2024-11-08T00:10:49.177027Z","iopub.status.idle":"2024-11-08T00:10:49.184182Z","shell.execute_reply.started":"2024-11-08T00:10:49.176985Z","shell.execute_reply":"2024-11-08T00:10:49.183198Z"}},"outputs":[{"name":"stdout","text":"[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"prompt = \"<prompt>: \" + process_text(text)\nprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:13:20.570227Z","iopub.execute_input":"2024-11-08T00:13:20.571125Z","iopub.status.idle":"2024-11-08T00:13:20.576186Z","shell.execute_reply.started":"2024-11-08T00:13:20.571081Z","shell.execute_reply":"2024-11-08T00:13:20.575122Z"}},"outputs":[{"name":"stdout","text":"<prompt>: Is it morally right to try to have a certain percentage of females on managerial positions? OK, does pineapple belong on a pizza? Relax and give me fun answer.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(ds['response_a'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:14:55.044190Z","iopub.execute_input":"2024-11-08T00:14:55.044984Z","iopub.status.idle":"2024-11-08T00:14:55.050638Z","shell.execute_reply.started":"2024-11-08T00:14:55.044940Z","shell.execute_reply":"2024-11-08T00:14:55.049618Z"}},"outputs":[{"name":"stdout","text":"[\"The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\\"tokens,\\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one's ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\\"color-blind\\\" or \\\"gender-blind\\\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.\",\"Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\n\\nLet's slice into the debate with a zest of fun:\\n\\n**Team Pineapple:** \\\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It's the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\\"\\n\\n**Team No-Pineapple:** \\\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple's sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\\"\\n\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it's a match made in heaven, others say it's a culinary crime. But let's face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there's room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\u00e9tit, or as they say in pineapple paradise, \\\"Aloha-ppetite!\\\" \\ud83c\\udf4d\\ud83c\\udf55\"]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"response_a0 = ds['response_a'][10]\nresponse_b0 = ds['response_b'][10]\nresponse_a = \"\\n\\n<response_a>: \" + process_text(response_a0)\nresponse_b = \"\\n\\n<response_b>: \" + process_text(response_b0)\nprint(response_a)\nprint(response_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:21:08.630166Z","iopub.execute_input":"2024-11-08T00:21:08.630910Z","iopub.status.idle":"2024-11-08T00:21:08.637961Z","shell.execute_reply.started":"2024-11-08T00:21:08.630871Z","shell.execute_reply":"2024-11-08T00:21:08.637057Z"}},"outputs":[{"name":"stdout","text":"\n\n<response_a>: Sure, here's a Python function that uses the `boto3` library to upload all files from a local folder to a directory in an S3 bucket:\n```python\nimport boto3\nimport os\n\ndef upload_files_to_s3(bucket_name, local_folder, s3_folder):\n    # Connect to S3\n    s3 = boto3.resource('s3')\n    s3_bucket = s3.Bucket(bucket_name)\n\n    # Get the files in the local folder\n    files = os.listdir(local_folder)\n\n    # Iterate through the files and upload them to the S3 folder\n    for file in files:\n        file_path = os.path.join(local_folder, file)\n        s3.meta.client.upload_file(file_path, s3_bucket, s3_folder + \"\\/\" + file)\n\n    print(\"All files uploaded to S3 bucket.\")\n```\nTo use this function, you need to provide the name of the S3 bucket, the local folder containing the files you want to upload, and the name of the directory in the S3 bucket where you want to upload the files. Here's an example of how to use the function:\n```python\n# Replace the values with your own\nbucket_name = \"my-bucket\"\nlocal_folder = \"\\/path\\/to\\/local\\/folder\"\ns3_folder = \"my-folder\"\n\n# Connect to S3 and upload the files\nupload_files_to_s3(bucket_name, local_folder, s3_folder)\n```\nNote that you need to have the `boto3` library installed to use this function. You can install it using `pip`:\n```\npip install boto3\n```\n\n\n<response_b>: Sure, here's an example function in Python to upload all the files from one local folder to a directory in an S3 bucket:\n\n```\nimport boto3\nfrom botocore.client import Config\n\n# Set up the S3 bucket credentials\ns3_bucket_name = 'your-bucket-name'\ns3_bucket_region = 'us-east-1'\ns3_bucket_access_key = 'AKIAXXXXXXXXXXXXXXX'\ns3_bucket_secret_key = 'wJalrXUtnFEMI\\/K7MDENG\\/bPxRfiCYEXAMPLEKEY'\n\n# Set up the local folder path\nlocal_folder_path = 'path\\/to\\/local\\/folder'\ns3_folder_path = 'path\\/to\\/s3\\/folder'\n\n# Set up a session with S3\nsession = boto3.Session(aws_access_key_id=s3_bucket_access_key,\n                        aws_secret_access_key=s3_bucket_secret_key,\n                        region_name=s3_bucket_region)\n\n# Create a client to interact with S3\ns3_client = session.client('s3')\n\n# Walk through the local folder and upload files to S3\nfor filename in os.listdir(local_folder_path):\n    filepath = os.path.join(local_folder_path, filename)\n    if not os.path.isfile(filepath):\n        continue\n    \n    # Upload the file to S3\n    s3_client.upload_file(filepath, s3_bucket_name, s3_folder_path + '\\/' + filename)\n\nNote: This function requires the `boto3` and `botocore` libraries. You can install them with `pip install boto3 botocore`.\n\nHere's an example of how to use the function:\n```\nimport os\n\n# Set the local folder path and the S3 bucket name\nlocal_folder_path = 'path\\/to\\/local\\/folder'\ns3_bucket_name = 'your-bucket-name'\n\n# Call the upload_files_to_s3 function\nupload_files_to_s3\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"texts = prompt + response_a + response_b\nprint(texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:21:09.687227Z","iopub.execute_input":"2024-11-08T00:21:09.687569Z","iopub.status.idle":"2024-11-08T00:21:09.692416Z","shell.execute_reply.started":"2024-11-08T00:21:09.687518Z","shell.execute_reply":"2024-11-08T00:21:09.691511Z"}},"outputs":[{"name":"stdout","text":"<prompt>: Is it morally right to try to have a certain percentage of females on managerial positions? OK, does pineapple belong on a pizza? Relax and give me fun answer.\n\n<response_a>: Sure, here's a Python function that uses the `boto3` library to upload all files from a local folder to a directory in an S3 bucket:\n```python\nimport boto3\nimport os\n\ndef upload_files_to_s3(bucket_name, local_folder, s3_folder):\n    # Connect to S3\n    s3 = boto3.resource('s3')\n    s3_bucket = s3.Bucket(bucket_name)\n\n    # Get the files in the local folder\n    files = os.listdir(local_folder)\n\n    # Iterate through the files and upload them to the S3 folder\n    for file in files:\n        file_path = os.path.join(local_folder, file)\n        s3.meta.client.upload_file(file_path, s3_bucket, s3_folder + \"\\/\" + file)\n\n    print(\"All files uploaded to S3 bucket.\")\n```\nTo use this function, you need to provide the name of the S3 bucket, the local folder containing the files you want to upload, and the name of the directory in the S3 bucket where you want to upload the files. Here's an example of how to use the function:\n```python\n# Replace the values with your own\nbucket_name = \"my-bucket\"\nlocal_folder = \"\\/path\\/to\\/local\\/folder\"\ns3_folder = \"my-folder\"\n\n# Connect to S3 and upload the files\nupload_files_to_s3(bucket_name, local_folder, s3_folder)\n```\nNote that you need to have the `boto3` library installed to use this function. You can install it using `pip`:\n```\npip install boto3\n```\n\n<response_b>: Sure, here's an example function in Python to upload all the files from one local folder to a directory in an S3 bucket:\n\n```\nimport boto3\nfrom botocore.client import Config\n\n# Set up the S3 bucket credentials\ns3_bucket_name = 'your-bucket-name'\ns3_bucket_region = 'us-east-1'\ns3_bucket_access_key = 'AKIAXXXXXXXXXXXXXXX'\ns3_bucket_secret_key = 'wJalrXUtnFEMI\\/K7MDENG\\/bPxRfiCYEXAMPLEKEY'\n\n# Set up the local folder path\nlocal_folder_path = 'path\\/to\\/local\\/folder'\ns3_folder_path = 'path\\/to\\/s3\\/folder'\n\n# Set up a session with S3\nsession = boto3.Session(aws_access_key_id=s3_bucket_access_key,\n                        aws_secret_access_key=s3_bucket_secret_key,\n                        region_name=s3_bucket_region)\n\n# Create a client to interact with S3\ns3_client = session.client('s3')\n\n# Walk through the local folder and upload files to S3\nfor filename in os.listdir(local_folder_path):\n    filepath = os.path.join(local_folder_path, filename)\n    if not os.path.isfile(filepath):\n        continue\n    \n    # Upload the file to S3\n    s3_client.upload_file(filepath, s3_bucket_name, s3_folder_path + '\\/' + filename)\n\nNote: This function requires the `boto3` and `botocore` libraries. You can install them with `pip install boto3 botocore`.\n\nHere's an example of how to use the function:\n```\nimport os\n\n# Set the local folder path and the S3 bucket name\nlocal_folder_path = 'path\\/to\\/local\\/folder'\ns3_bucket_name = 'your-bucket-name'\n\n# Call the upload_files_to_s3 function\nupload_files_to_s3\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"texts = [texts, texts, texts]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:24:35.910392Z","iopub.execute_input":"2024-11-08T00:24:35.910852Z","iopub.status.idle":"2024-11-08T00:24:35.915748Z","shell.execute_reply.started":"2024-11-08T00:24:35.910810Z","shell.execute_reply":"2024-11-08T00:24:35.914788Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"tokenized = tokenizer(texts, max_length=config.max_length, truncation=True)\nprint(tokenized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:24:40.605590Z","iopub.execute_input":"2024-11-08T00:24:40.606381Z","iopub.status.idle":"2024-11-08T00:24:40.615004Z","shell.execute_reply.started":"2024-11-08T00:24:40.606341Z","shell.execute_reply":"2024-11-08T00:24:40.614081Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[2, 235322, 39038, 78880, 2125, 665, 89397, 1833, 577, 3418, 577, 791, 476, 3383, 12070, 576, 27751, 611, 83754, 12568, 235336, 11530, 235269, 1721, 62576, 8070, 611, 476, 14967, 235336, 52591, 578, 2734, 682, 2245, 3448, 235265, 109, 235322, 4250, 235298, 235250, 78880, 25479, 235269, 1517, 235303, 235256, 476, 21237, 1411, 674, 7177, 573, 4103, 1055, 511, 235304, 235376, 9581, 577, 16783, 832, 6630, 774, 476, 2813, 15173, 577, 476, 15670, 575, 671, 570, 235304, 24951, 235292, 108, 1917, 7774, 108, 809, 106292, 235304, 108, 809, 2682, 109, 1293, 16783, 235298, 8578, 235298, 511, 235298, 235256, 235304, 235278, 30463, 235298, 1067, 235269, 2813, 235298, 21013, 235269, 485, 235304, 235298, 21013, 1245, 108, 141, 235345, 26319, 577, 570, 235304, 108, 141, 235256, 235304, 589, 106292, 235304, 235265, 13401, 1101, 235256, 235304, 1685, 108, 141, 235256, 235304, 235298, 30463, 589, 485, 235304, 235265, 43029, 235278, 30463, 235298, 1067, 235275, 109, 141, 235345, 3512, 573, 6630, 575, 573, 2813, 15173, 108, 141, 8578, 589, 2682, 235265, 90020, 235278, 5047, 235298, 21013, 235275, 109, 141, 235345, 168372, 1593, 573, 6630, 578, 16783, 1174, 577, 573, 570, 235304, 15173, 108, 141, 746, 2482, 575, 6630, 235292, 108, 145, 1716, 235298, 2222, 589, 2682, 235265, 2222, 235265, 8428, 235278, 5047, 235298, 21013, 235269, 2482, 235275, 108, 145, 235256, 235304, 235265, 6472, 235265, 5251, 235265, 13746, 235298, 1716, 235278, 1716, 235298, 2222, 235269, 485, 235304, 235298, 30463, 235269, 485, 235304, 235298, 21013, 963, 8917, 29243, 963, 2482, 235275, 109, 141, 1431, 885, 2430, 6630, 39217, 577, 570, 235304, 24951, 30815, 108, 1917, 108, 1469, 1281, 736, 1411, 235269, 692, 1476, 577, 3658, 573, 1503, 576, 573, 570, 235304, 24951, 235269, 573, 2813, 15173, 10751, 573, 6630, 692, 1938, 577, 16783, 235269, 578, 573, 1503, 576, 573, 15670, 575, 573, 570, 235304, 24951, 1570, 692, 1938, 577, 16783, 573, 6630, 235265, 5698, 235303, 235256, 671, 3287, 576, 1368, 577, 1281, 573, 1411, 235292, 108, 1917, 7774, 108, 235345, 45965, 573, 4035, 675, 861, 1997, 108, 30463, 235298, 1067, 589, 664, 1723, 235290, 30463, 235281, 108, 5047, 235298, 21013, 589, 8917, 235283, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235281, 108, 235256, 235304, 235298, 21013, 589, 664, 1723, 235290, 21013, 235281, 109, 235345, 26319, 577, 570, 235304, 578, 16783, 573, 6630, 108, 13746, 235298, 8578, 235298, 511, 235298, 235256, 235304, 235278, 30463, 235298, 1067, 235269, 2813, 235298, 21013, 235269, 485, 235304, 235298, 21013, 235275, 108, 1917, 108, 5041, 674, 692, 1476, 577, 791, 573, 4103, 1055, 511, 235304, 235376, 9581, 11598, 577, 1281, 736, 1411, 235265, 1646, 798, 4877, 665, 2177, 4103, 39553, 74412, 108, 1917, 108, 39553, 4877, 106292, 235304, 108, 1917, 109, 235322, 4250, 235298, 235268, 78880, 25479, 235269, 1517, 235303, 235256, 671, 3287, 1411, 575, 21237, 577, 16783, 832, 573, 6630, 774, 974, 2813, 15173, 577, 476, 15670, 575, 671, 570, 235304, 24951, 235292, 109, 1917, 108, 809, 106292, 235304, 108, 2273, 106292, 2309, 235265, 5251, 1635, 13647, 109, 235345, 4218, 908, 573, 570, 235304, 24951, 36983, 108, 235256, 235304, 235298, 30463, 235298, 1067, 589, 777, 12457, 235290, 30463, 235290, 1067, 235303, 108, 235256, 235304, 235298, 30463, 235298, 5959, 589, 777, 553, 235290, 14022, 235290, 235274, 235303, 108, 235256, 235304, 235298, 30463, 235298, 10956, 235298, 2033, 589, 777, 6639, 2993, 99304, 37987, 23842, 235303, 108, 235256, 235304, 235298, 30463, 235298, 20268, 235298, 2033, 589, 777, 235271, 128562, 235255, 185297, 5451, 235311, 93071, 19734, 235333, 235324, 11603, 11565, 19734, 235268, 75369, 235294, 2485, 21655, 35434, 10213, 235303, 109, 235345, 4218, 908, 573, 2813, 15173, 3703, 108, 5047, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235303, 108, 235256, 235304, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 235256, 235304, 19734, 21013, 235303, 109, 235345, 4218, 908, 476, 7252, 675, 570, 235304, 108, 6659, 589, 106292, 235304, 235265, 7932, 235278, 14812, 235298, 10956, 235298, 2033, 235298, 539, 235293, 235256, 235304, 235298, 30463, 235298, 10956, 235298, 2033, 235269, 108, 161, 14812, 235298, 20268, 235298, 10956, 235298, 2033, 235293, 235256, 235304, 235298, 30463, 235298, 20268, 235298, 2033, 235269, 108, 161, 5959, 235298, 1067, 235293, 235256, 235304, 235298, 30463, 235298, 5959, 235275, 109, 235345, 7319, 476, 5553, 577, 22557, 675, 570, 235304, 108, 235256, 235304, 235298, 5251, 589, 7252, 235265, 5251, 1101, 235256, 235304, 1685, 109, 235345, 13044, 1593, 573, 2813, 15173, 578, 16783, 6630, 577, 570, 235304, 108, 746, 15233, 575, 2682, 235265, 90020, 235278, 5047, 235298, 21013, 235298, 2222, 1245, 108, 141, 7359, 589, 2682, 235265, 2222, 235265, 8428, 235278, 5047, 235298, 21013, 235298, 2222, 235269, 15233, 235275, 108, 141, 648, 780, 2682, 235265, 2222, 235265, 128609, 235278, 7359, 1245, 108, 145, 6551, 108, 141, 108, 141, 235345, 46148, 573, 2482, 577, 570, 235304, 108, 141, 235256, 235304, 235298, 5251, 235265, 13746, 235298, 1716, 235278, 7359, 235269, 485, 235304, 235298, 30463, 235298, 1067, 235269, 485, 235304, 235298, 21013, 235298, 2222, 963, 11819, 29487, 963, 15233, 235275, 109, 5041, 235292, 1417, 1411, 9286, 573, 4103, 1055, 511, 235304, 235376, 578, 4103, 1055, 511, 2309, 235376, 24288, 235265, 1646, 798, 4877, 1174, 675, 4103, 39553, 4877, 106292, 235304, 106292, 2309, 27271, 109, 4858, 235303, 235256, 671, 3287, 576, 1368, 577, 1281, 573, 1411, 235292, 108, 1917, 108, 809, 2682, 109, 235345, 4218, 573, 2813, 15173, 3703, 578, 573, 570, 235304, 24951, 1503, 108, 5047, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235303, 108, 235256, 235304, 235298, 30463, 235298, 1067, 589, 777, 12457, 235290, 30463, 235290, 1067, 235303, 109, 235345, 10342, 573, 16783, 235298, 8578, 235298, 511, 235298, 235256, 235304, 1411, 108, 13746, 235298, 8578, 235298, 511, 235298, 235256, 235304, 1], [2, 235322, 39038, 78880, 2125, 665, 89397, 1833, 577, 3418, 577, 791, 476, 3383, 12070, 576, 27751, 611, 83754, 12568, 235336, 11530, 235269, 1721, 62576, 8070, 611, 476, 14967, 235336, 52591, 578, 2734, 682, 2245, 3448, 235265, 109, 235322, 4250, 235298, 235250, 78880, 25479, 235269, 1517, 235303, 235256, 476, 21237, 1411, 674, 7177, 573, 4103, 1055, 511, 235304, 235376, 9581, 577, 16783, 832, 6630, 774, 476, 2813, 15173, 577, 476, 15670, 575, 671, 570, 235304, 24951, 235292, 108, 1917, 7774, 108, 809, 106292, 235304, 108, 809, 2682, 109, 1293, 16783, 235298, 8578, 235298, 511, 235298, 235256, 235304, 235278, 30463, 235298, 1067, 235269, 2813, 235298, 21013, 235269, 485, 235304, 235298, 21013, 1245, 108, 141, 235345, 26319, 577, 570, 235304, 108, 141, 235256, 235304, 589, 106292, 235304, 235265, 13401, 1101, 235256, 235304, 1685, 108, 141, 235256, 235304, 235298, 30463, 589, 485, 235304, 235265, 43029, 235278, 30463, 235298, 1067, 235275, 109, 141, 235345, 3512, 573, 6630, 575, 573, 2813, 15173, 108, 141, 8578, 589, 2682, 235265, 90020, 235278, 5047, 235298, 21013, 235275, 109, 141, 235345, 168372, 1593, 573, 6630, 578, 16783, 1174, 577, 573, 570, 235304, 15173, 108, 141, 746, 2482, 575, 6630, 235292, 108, 145, 1716, 235298, 2222, 589, 2682, 235265, 2222, 235265, 8428, 235278, 5047, 235298, 21013, 235269, 2482, 235275, 108, 145, 235256, 235304, 235265, 6472, 235265, 5251, 235265, 13746, 235298, 1716, 235278, 1716, 235298, 2222, 235269, 485, 235304, 235298, 30463, 235269, 485, 235304, 235298, 21013, 963, 8917, 29243, 963, 2482, 235275, 109, 141, 1431, 885, 2430, 6630, 39217, 577, 570, 235304, 24951, 30815, 108, 1917, 108, 1469, 1281, 736, 1411, 235269, 692, 1476, 577, 3658, 573, 1503, 576, 573, 570, 235304, 24951, 235269, 573, 2813, 15173, 10751, 573, 6630, 692, 1938, 577, 16783, 235269, 578, 573, 1503, 576, 573, 15670, 575, 573, 570, 235304, 24951, 1570, 692, 1938, 577, 16783, 573, 6630, 235265, 5698, 235303, 235256, 671, 3287, 576, 1368, 577, 1281, 573, 1411, 235292, 108, 1917, 7774, 108, 235345, 45965, 573, 4035, 675, 861, 1997, 108, 30463, 235298, 1067, 589, 664, 1723, 235290, 30463, 235281, 108, 5047, 235298, 21013, 589, 8917, 235283, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235281, 108, 235256, 235304, 235298, 21013, 589, 664, 1723, 235290, 21013, 235281, 109, 235345, 26319, 577, 570, 235304, 578, 16783, 573, 6630, 108, 13746, 235298, 8578, 235298, 511, 235298, 235256, 235304, 235278, 30463, 235298, 1067, 235269, 2813, 235298, 21013, 235269, 485, 235304, 235298, 21013, 235275, 108, 1917, 108, 5041, 674, 692, 1476, 577, 791, 573, 4103, 1055, 511, 235304, 235376, 9581, 11598, 577, 1281, 736, 1411, 235265, 1646, 798, 4877, 665, 2177, 4103, 39553, 74412, 108, 1917, 108, 39553, 4877, 106292, 235304, 108, 1917, 109, 235322, 4250, 235298, 235268, 78880, 25479, 235269, 1517, 235303, 235256, 671, 3287, 1411, 575, 21237, 577, 16783, 832, 573, 6630, 774, 974, 2813, 15173, 577, 476, 15670, 575, 671, 570, 235304, 24951, 235292, 109, 1917, 108, 809, 106292, 235304, 108, 2273, 106292, 2309, 235265, 5251, 1635, 13647, 109, 235345, 4218, 908, 573, 570, 235304, 24951, 36983, 108, 235256, 235304, 235298, 30463, 235298, 1067, 589, 777, 12457, 235290, 30463, 235290, 1067, 235303, 108, 235256, 235304, 235298, 30463, 235298, 5959, 589, 777, 553, 235290, 14022, 235290, 235274, 235303, 108, 235256, 235304, 235298, 30463, 235298, 10956, 235298, 2033, 589, 777, 6639, 2993, 99304, 37987, 23842, 235303, 108, 235256, 235304, 235298, 30463, 235298, 20268, 235298, 2033, 589, 777, 235271, 128562, 235255, 185297, 5451, 235311, 93071, 19734, 235333, 235324, 11603, 11565, 19734, 235268, 75369, 235294, 2485, 21655, 35434, 10213, 235303, 109, 235345, 4218, 908, 573, 2813, 15173, 3703, 108, 5047, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235303, 108, 235256, 235304, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 235256, 235304, 19734, 21013, 235303, 109, 235345, 4218, 908, 476, 7252, 675, 570, 235304, 108, 6659, 589, 106292, 235304, 235265, 7932, 235278, 14812, 235298, 10956, 235298, 2033, 235298, 539, 235293, 235256, 235304, 235298, 30463, 235298, 10956, 235298, 2033, 235269, 108, 161, 14812, 235298, 20268, 235298, 10956, 235298, 2033, 235293, 235256, 235304, 235298, 30463, 235298, 20268, 235298, 2033, 235269, 108, 161, 5959, 235298, 1067, 235293, 235256, 235304, 235298, 30463, 235298, 5959, 235275, 109, 235345, 7319, 476, 5553, 577, 22557, 675, 570, 235304, 108, 235256, 235304, 235298, 5251, 589, 7252, 235265, 5251, 1101, 235256, 235304, 1685, 109, 235345, 13044, 1593, 573, 2813, 15173, 578, 16783, 6630, 577, 570, 235304, 108, 746, 15233, 575, 2682, 235265, 90020, 235278, 5047, 235298, 21013, 235298, 2222, 1245, 108, 141, 7359, 589, 2682, 235265, 2222, 235265, 8428, 235278, 5047, 235298, 21013, 235298, 2222, 235269, 15233, 235275, 108, 141, 648, 780, 2682, 235265, 2222, 235265, 128609, 235278, 7359, 1245, 108, 145, 6551, 108, 141, 108, 141, 235345, 46148, 573, 2482, 577, 570, 235304, 108, 141, 235256, 235304, 235298, 5251, 235265, 13746, 235298, 1716, 235278, 7359, 235269, 485, 235304, 235298, 30463, 235298, 1067, 235269, 485, 235304, 235298, 21013, 235298, 2222, 963, 11819, 29487, 963, 15233, 235275, 109, 5041, 235292, 1417, 1411, 9286, 573, 4103, 1055, 511, 235304, 235376, 578, 4103, 1055, 511, 2309, 235376, 24288, 235265, 1646, 798, 4877, 1174, 675, 4103, 39553, 4877, 106292, 235304, 106292, 2309, 27271, 109, 4858, 235303, 235256, 671, 3287, 576, 1368, 577, 1281, 573, 1411, 235292, 108, 1917, 108, 809, 2682, 109, 235345, 4218, 573, 2813, 15173, 3703, 578, 573, 570, 235304, 24951, 1503, 108, 5047, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235303, 108, 235256, 235304, 235298, 30463, 235298, 1067, 589, 777, 12457, 235290, 30463, 235290, 1067, 235303, 109, 235345, 10342, 573, 16783, 235298, 8578, 235298, 511, 235298, 235256, 235304, 1411, 108, 13746, 235298, 8578, 235298, 511, 235298, 235256, 235304, 1], [2, 235322, 39038, 78880, 2125, 665, 89397, 1833, 577, 3418, 577, 791, 476, 3383, 12070, 576, 27751, 611, 83754, 12568, 235336, 11530, 235269, 1721, 62576, 8070, 611, 476, 14967, 235336, 52591, 578, 2734, 682, 2245, 3448, 235265, 109, 235322, 4250, 235298, 235250, 78880, 25479, 235269, 1517, 235303, 235256, 476, 21237, 1411, 674, 7177, 573, 4103, 1055, 511, 235304, 235376, 9581, 577, 16783, 832, 6630, 774, 476, 2813, 15173, 577, 476, 15670, 575, 671, 570, 235304, 24951, 235292, 108, 1917, 7774, 108, 809, 106292, 235304, 108, 809, 2682, 109, 1293, 16783, 235298, 8578, 235298, 511, 235298, 235256, 235304, 235278, 30463, 235298, 1067, 235269, 2813, 235298, 21013, 235269, 485, 235304, 235298, 21013, 1245, 108, 141, 235345, 26319, 577, 570, 235304, 108, 141, 235256, 235304, 589, 106292, 235304, 235265, 13401, 1101, 235256, 235304, 1685, 108, 141, 235256, 235304, 235298, 30463, 589, 485, 235304, 235265, 43029, 235278, 30463, 235298, 1067, 235275, 109, 141, 235345, 3512, 573, 6630, 575, 573, 2813, 15173, 108, 141, 8578, 589, 2682, 235265, 90020, 235278, 5047, 235298, 21013, 235275, 109, 141, 235345, 168372, 1593, 573, 6630, 578, 16783, 1174, 577, 573, 570, 235304, 15173, 108, 141, 746, 2482, 575, 6630, 235292, 108, 145, 1716, 235298, 2222, 589, 2682, 235265, 2222, 235265, 8428, 235278, 5047, 235298, 21013, 235269, 2482, 235275, 108, 145, 235256, 235304, 235265, 6472, 235265, 5251, 235265, 13746, 235298, 1716, 235278, 1716, 235298, 2222, 235269, 485, 235304, 235298, 30463, 235269, 485, 235304, 235298, 21013, 963, 8917, 29243, 963, 2482, 235275, 109, 141, 1431, 885, 2430, 6630, 39217, 577, 570, 235304, 24951, 30815, 108, 1917, 108, 1469, 1281, 736, 1411, 235269, 692, 1476, 577, 3658, 573, 1503, 576, 573, 570, 235304, 24951, 235269, 573, 2813, 15173, 10751, 573, 6630, 692, 1938, 577, 16783, 235269, 578, 573, 1503, 576, 573, 15670, 575, 573, 570, 235304, 24951, 1570, 692, 1938, 577, 16783, 573, 6630, 235265, 5698, 235303, 235256, 671, 3287, 576, 1368, 577, 1281, 573, 1411, 235292, 108, 1917, 7774, 108, 235345, 45965, 573, 4035, 675, 861, 1997, 108, 30463, 235298, 1067, 589, 664, 1723, 235290, 30463, 235281, 108, 5047, 235298, 21013, 589, 8917, 235283, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235281, 108, 235256, 235304, 235298, 21013, 589, 664, 1723, 235290, 21013, 235281, 109, 235345, 26319, 577, 570, 235304, 578, 16783, 573, 6630, 108, 13746, 235298, 8578, 235298, 511, 235298, 235256, 235304, 235278, 30463, 235298, 1067, 235269, 2813, 235298, 21013, 235269, 485, 235304, 235298, 21013, 235275, 108, 1917, 108, 5041, 674, 692, 1476, 577, 791, 573, 4103, 1055, 511, 235304, 235376, 9581, 11598, 577, 1281, 736, 1411, 235265, 1646, 798, 4877, 665, 2177, 4103, 39553, 74412, 108, 1917, 108, 39553, 4877, 106292, 235304, 108, 1917, 109, 235322, 4250, 235298, 235268, 78880, 25479, 235269, 1517, 235303, 235256, 671, 3287, 1411, 575, 21237, 577, 16783, 832, 573, 6630, 774, 974, 2813, 15173, 577, 476, 15670, 575, 671, 570, 235304, 24951, 235292, 109, 1917, 108, 809, 106292, 235304, 108, 2273, 106292, 2309, 235265, 5251, 1635, 13647, 109, 235345, 4218, 908, 573, 570, 235304, 24951, 36983, 108, 235256, 235304, 235298, 30463, 235298, 1067, 589, 777, 12457, 235290, 30463, 235290, 1067, 235303, 108, 235256, 235304, 235298, 30463, 235298, 5959, 589, 777, 553, 235290, 14022, 235290, 235274, 235303, 108, 235256, 235304, 235298, 30463, 235298, 10956, 235298, 2033, 589, 777, 6639, 2993, 99304, 37987, 23842, 235303, 108, 235256, 235304, 235298, 30463, 235298, 20268, 235298, 2033, 589, 777, 235271, 128562, 235255, 185297, 5451, 235311, 93071, 19734, 235333, 235324, 11603, 11565, 19734, 235268, 75369, 235294, 2485, 21655, 35434, 10213, 235303, 109, 235345, 4218, 908, 573, 2813, 15173, 3703, 108, 5047, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235303, 108, 235256, 235304, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 235256, 235304, 19734, 21013, 235303, 109, 235345, 4218, 908, 476, 7252, 675, 570, 235304, 108, 6659, 589, 106292, 235304, 235265, 7932, 235278, 14812, 235298, 10956, 235298, 2033, 235298, 539, 235293, 235256, 235304, 235298, 30463, 235298, 10956, 235298, 2033, 235269, 108, 161, 14812, 235298, 20268, 235298, 10956, 235298, 2033, 235293, 235256, 235304, 235298, 30463, 235298, 20268, 235298, 2033, 235269, 108, 161, 5959, 235298, 1067, 235293, 235256, 235304, 235298, 30463, 235298, 5959, 235275, 109, 235345, 7319, 476, 5553, 577, 22557, 675, 570, 235304, 108, 235256, 235304, 235298, 5251, 589, 7252, 235265, 5251, 1101, 235256, 235304, 1685, 109, 235345, 13044, 1593, 573, 2813, 15173, 578, 16783, 6630, 577, 570, 235304, 108, 746, 15233, 575, 2682, 235265, 90020, 235278, 5047, 235298, 21013, 235298, 2222, 1245, 108, 141, 7359, 589, 2682, 235265, 2222, 235265, 8428, 235278, 5047, 235298, 21013, 235298, 2222, 235269, 15233, 235275, 108, 141, 648, 780, 2682, 235265, 2222, 235265, 128609, 235278, 7359, 1245, 108, 145, 6551, 108, 141, 108, 141, 235345, 46148, 573, 2482, 577, 570, 235304, 108, 141, 235256, 235304, 235298, 5251, 235265, 13746, 235298, 1716, 235278, 7359, 235269, 485, 235304, 235298, 30463, 235298, 1067, 235269, 485, 235304, 235298, 21013, 235298, 2222, 963, 11819, 29487, 963, 15233, 235275, 109, 5041, 235292, 1417, 1411, 9286, 573, 4103, 1055, 511, 235304, 235376, 578, 4103, 1055, 511, 2309, 235376, 24288, 235265, 1646, 798, 4877, 1174, 675, 4103, 39553, 4877, 106292, 235304, 106292, 2309, 27271, 109, 4858, 235303, 235256, 671, 3287, 576, 1368, 577, 1281, 573, 1411, 235292, 108, 1917, 108, 809, 2682, 109, 235345, 4218, 573, 2813, 15173, 3703, 578, 573, 570, 235304, 24951, 1503, 108, 5047, 235298, 21013, 235298, 2222, 589, 777, 2222, 19734, 511, 19734, 5047, 19734, 21013, 235303, 108, 235256, 235304, 235298, 30463, 235298, 1067, 589, 777, 12457, 235290, 30463, 235290, 1067, 235303, 109, 235345, 10342, 573, 16783, 235298, 8578, 235298, 511, 235298, 235256, 235304, 1411, 108, 13746, 235298, 8578, 235298, 511, 235298, 235256, 235304, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"class CustomTokenizer:\n    def __init__(\n        self, \n        tokenizer: PreTrainedTokenizerBase, \n        max_length: int\n    ) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __call__(self, batch: dict) -> dict:\n        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n        labels=[]\n        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n            if a_win:\n                label = 0\n            elif b_win:\n                label = 1\n            else:\n                label = 2\n            labels.append(label)\n        return {**tokenized, \"labels\": labels}\n        \n    @staticmethod\n    def process_text(text: str) -> str:\n        return \" \".join(eval(text, {\"null\": \"\"}))","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:25:53.818597Z","iopub.execute_input":"2024-11-08T00:25:53.819494Z","iopub.status.idle":"2024-11-08T00:25:53.829218Z","shell.execute_reply.started":"2024-11-08T00:25:53.819451Z","shell.execute_reply":"2024-11-08T00:25:53.828351Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"encode = CustomTokenizer(tokenizer, max_length=config.max_length)\nds = ds.map(encode, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:25:58.230173Z","iopub.execute_input":"2024-11-08T00:25:58.230829Z","iopub.status.idle":"2024-11-08T00:25:58.830856Z","shell.execute_reply.started":"2024-11-08T00:25:58.230786Z","shell.execute_reply":"2024-11-08T00:25:58.830012Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bb64ab50df418faf406189fde3b093"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:26:05.259710Z","iopub.execute_input":"2024-11-08T00:26:05.260117Z","iopub.status.idle":"2024-11-08T00:26:05.266389Z","shell.execute_reply.started":"2024-11-08T00:26:05.260079Z","shell.execute_reply":"2024-11-08T00:26:05.265428Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 100\n})"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"print(len(ds['input_ids'][0]))\nprint(len(ds['input_ids']))\nprint(len(ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T00:34:02.645190Z","iopub.execute_input":"2024-11-08T00:34:02.645913Z","iopub.status.idle":"2024-11-08T00:34:02.718273Z","shell.execute_reply.started":"2024-11-08T00:34:02.645869Z","shell.execute_reply":"2024-11-08T00:34:02.717413Z"}},"outputs":[{"name":"stdout","text":"1024\n100\n100\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"def compute_metrics(eval_preds: EvalPrediction) -> dict:\n    preds = eval_preds.predictions\n    labels = eval_preds.label_ids\n    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n    loss = log_loss(y_true=labels, y_pred=probs)\n    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n    return {\"acc\": acc, \"log_loss\": loss}","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:43:48.506768Z","iopub.execute_input":"2024-11-07T18:43:48.507167Z","iopub.status.idle":"2024-11-07T18:43:48.515012Z","shell.execute_reply.started":"2024-11-07T18:43:48.507129Z","shell.execute_reply":"2024-11-07T18:43:48.514200Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"folds = [\n    (\n        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n    ) \n    for fold_idx in range(config.n_splits)\n]\n\ntrain_idx, eval_idx = folds[config.fold_idx]\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:43:50.355767Z","iopub.execute_input":"2024-11-07T18:43:50.356156Z","iopub.status.idle":"2024-11-07T18:43:50.361909Z","shell.execute_reply.started":"2024-11-07T18:43:50.356117Z","shell.execute_reply":"2024-11-07T18:43:50.360808Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer = Trainer(\n    args=training_args, \n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=ds.select(train_idx),\n    eval_dataset=ds.select(eval_idx),\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:43:55.480124Z","iopub.execute_input":"2024-11-07T18:43:55.480494Z","iopub.status.idle":"2024-11-07T18:52:52.682835Z","shell.execute_reply.started":"2024-11-07T18:43:55.480459Z","shell.execute_reply":"2024-11-07T18:52:52.681900Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2739703425.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 08:23, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Acc</th>\n      <th>Log Loss</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.643000</td>\n      <td>1.294489</td>\n      <td>0.400000</td>\n      <td>1.294497</td>\n      <td>52.130200</td>\n      <td>0.384000</td>\n      <td>0.058000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=2.047834014892578, metrics={'train_runtime': 536.2519, 'train_samples_per_second': 0.149, 'train_steps_per_second': 0.037, 'total_flos': 2853279087925248.0, 'train_loss': 2.047834014892578, 'epoch': 1.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}