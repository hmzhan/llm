{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7711309,"sourceType":"datasetVersion","datasetId":4484051},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nIn this notebook we demostrate how to finetune **Llama3** (8B) model from Meta using **QLoRA** and **SFTTrainer** from **tlr**.\n\n## What is Llama3?\n\nLlama3 is the latest release of open-source LLMs from Meta, with features pretrained and instruction-fine-tuned language models with 8B and 70B parameters.\n\n## What is LoRA?\n\nLoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times.\n\n\n## What is QLoRA?\n\nQLoRA builds on LoRA by incorporating quantization techniques to further reduce memory usage while maintaining, or even enhancing, model performance. With QLoRA it is possible to finetune a 70B parameter model that requires 36 GPUs with only 2!\n\n\n## What is PEFT?\n\nParameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing (NLP) to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained model’s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task.\n\n## What is SFTTrainer?\n\nSFT in SFTTrainer stands for supervised fine-tuning. The **trl** (Transformer Reinforcement Learning) library from HuggingFace provides a simple API to fine-tune models using SFTTrainer.\n\n## What is UltraChat200k?  \n\nUltraChat-200k is an invaluable resource for natural language understanding, generation and dialog system research. With 1.4 million dialogues spanning a variety of topics, this parquet-formatted dataset offers researchers four distinct formats to aid in their studies: test_sft, train_sft, train_gen and test_gen. More details [here](https://www.kaggle.com/datasets/thedevastator/ultrachat-200k-nlp-dataset).\n\n## Inspiration\n\nFor this notebook, I took inspiration from several sources:\n* [Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora](https://www.philschmid.de/fsdp-qlora-llama3)  \n* [Fine-tuning LLMs using LoRA](https://medium.com/@rajatsharma_33357/fine-tuning-llama-using-lora-fb3f48a557d5)  \n* [Fine-tuning Llama-3–8B-Instruct QLORA using low cost resources](https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04)  \n* [Llama2 Fine-Tuning with Low-Rank Adaptations (LoRA) on Gaudi 2 Processors](https://eduand-alvarez.medium.com/llama2-fine-tuning-with-low-rank-adaptations-lora-on-gaudi-2-processors-52cf1ee6ce11)  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Install and import libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U transformers\n!pip install -q -U peft\n!pip install -q -U accelerate\n!pip install -q -U datasets\n!pip install -q -U trl","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb_api\")\nimport wandb\n! wandb login $wandb_key","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:40:56.036087Z","iopub.execute_input":"2024-05-04T12:40:56.036459Z","iopub.status.idle":"2024-05-04T12:41:00.330902Z","shell.execute_reply.started":"2024-05-04T12:40:56.036434Z","shell.execute_reply":"2024-05-04T12:41:00.329817Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom time import time\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer,setup_chat_format","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-04T12:41:00.334375Z","iopub.execute_input":"2024-05-04T12:41:00.335167Z","iopub.status.idle":"2024-05-04T12:41:20.773051Z","shell.execute_reply.started":"2024-05-04T12:41:00.335131Z","shell.execute_reply":"2024-05-04T12:41:20.772242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize model\n\n\nThe model used is:\n\n* **Model**: Llama3  \n* **Framework**: Transformers   \n* **Size**: 8B   \n* **Type**: 8b-chat-hf (hf stands for HuggingFace). \n* **Version**: V1  ","metadata":{}},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:41:20.774166Z","iopub.execute_input":"2024-05-04T12:41:20.774738Z","iopub.status.idle":"2024-05-04T12:41:20.779313Z","shell.execute_reply.started":"2024-05-04T12:41:20.77471Z","shell.execute_reply":"2024-05-04T12:41:20.778263Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are using quantization (with BitsAndBytes).","metadata":{}},{"cell_type":"code","source":"compute_dtype = torch.bfloat16\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:41:20.780731Z","iopub.execute_input":"2024-05-04T12:41:20.781017Z","iopub.status.idle":"2024-05-04T12:41:20.826571Z","shell.execute_reply.started":"2024-05-04T12:41:20.780993Z","shell.execute_reply":"2024-05-04T12:41:20.825757Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We define the model configuration, the model (using AutoModelForCausalLM) and the tokenizer (using AutoTokenizer).","metadata":{}},{"cell_type":"code","source":"time_start = time()\n\nmodel_config = AutoConfig.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    max_new_tokens=1024\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_end = time()\nprint(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:41:20.827623Z","iopub.execute_input":"2024-05-04T12:41:20.827871Z","iopub.status.idle":"2024-05-04T12:43:39.621551Z","shell.execute_reply.started":"2024-05-04T12:41:20.82785Z","shell.execute_reply":"2024-05-04T12:43:39.620635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:43:39.622964Z","iopub.execute_input":"2024-05-04T12:43:39.623767Z","iopub.status.idle":"2024-05-04T12:43:39.743529Z","shell.execute_reply.started":"2024-05-04T12:43:39.623731Z","shell.execute_reply":"2024-05-04T12:43:39.74277Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare the dataset   \n\n\nWe will use 10K rows from the `ultrachat_200k` database.","metadata":{}},{"cell_type":"code","source":"dataset_name = \"HuggingFaceH4/ultrachat_200k\"\ndataset = load_dataset(dataset_name, split=\"train_sft\")\ndataset = dataset.shuffle(seed=42).select(range(10000))\n\ndef format_chat_template(row):\n    chat = tokenizer.apply_chat_template(row[\"messages\"], tokenize=False)\n    return {\"text\":chat}\n\nprocessed_dataset = dataset.map(\n    format_chat_template,\n    num_proc= os.cpu_count(),\n)\n\ndataset = processed_dataset.train_test_split(test_size=0.01)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:43:39.744558Z","iopub.execute_input":"2024-05-04T12:43:39.744832Z","iopub.status.idle":"2024-05-04T12:44:16.642092Z","shell.execute_reply.started":"2024-05-04T12:43:39.744809Z","shell.execute_reply":"2024-05-04T12:44:16.640735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Training configuration   \n\nWe set the rank for LoRA to 4, to reduce the number of trainable parameters.","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n        lora_alpha=64,\n        lora_dropout=0.05,\n        r=4,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:44:16.647115Z","iopub.execute_input":"2024-05-04T12:44:16.647615Z","iopub.status.idle":"2024-05-04T12:44:16.65564Z","shell.execute_reply.started":"2024-05-04T12:44:16.647565Z","shell.execute_reply":"2024-05-04T12:44:16.654533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n        output_dir=\"./results_llama3_sft/\",\n        evaluation_strategy=\"steps\",\n        do_eval=True,\n        optim=\"paged_adamw_8bit\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=2,\n        per_device_eval_batch_size=8,\n        log_level=\"debug\",\n        save_steps=20,\n        logging_steps=1,\n        learning_rate=8e-6,\n        eval_steps=1,\n        max_steps=20,\n        num_train_epochs=20,\n        warmup_steps=3,\n        lr_scheduler_type=\"linear\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:44:33.142786Z","iopub.execute_input":"2024-05-04T12:44:33.14315Z","iopub.status.idle":"2024-05-04T12:44:33.187137Z","shell.execute_reply.started":"2024-05-04T12:44:33.143121Z","shell.execute_reply":"2024-05-04T12:44:33.186236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training process","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:44:36.659592Z","iopub.execute_input":"2024-05-04T12:44:36.660313Z","iopub.status.idle":"2024-05-04T12:44:36.664509Z","shell.execute_reply.started":"2024-05-04T12:44:36.66028Z","shell.execute_reply":"2024-05-04T12:44:36.663437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset['train'],\n        eval_dataset=dataset['test'],\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        tokenizer=tokenizer,\n        args=training_arguments,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:44:38.350615Z","iopub.execute_input":"2024-05-04T12:44:38.351518Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusions\n\nBy reducing the batch_size, max_seq_length and the LoRA rank, we managed to run the Llama3 fine-tunning with QLoRA in the Kaggle environment.\n\n**Note**: we set `os.environ[\"WANDB_DISABLED\"]` to `True` and `save_steps` to `20`. We intend to save the last checkpoint as a **Kaggle Model**.\n","metadata":{}}]}